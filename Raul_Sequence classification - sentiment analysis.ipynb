{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11QRgS_w_phqFq5cMC464yDYzrGqolh_D","timestamp":1715069544759}],"gpuType":"T4","collapsed_sections":["9yYKhKywFv45","b8IOXytCBJdu","u2unc2ziBhBg","PeaIHk8TH2wa","GV5GqsgQIjrl","2O0bRxt_F4Zd"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# LOADS"],"metadata":{"id":"9yYKhKywFv45"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3EmaV-rSzgf","executionInfo":{"status":"ok","timestamp":1716713358136,"user_tz":-120,"elapsed":21460,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"558fc6d8-5458-4efb-fd3d-d430255b89a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["%cd \"/content/gdrive/Shareddrives/NLP_Lab_3\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keEfeSJx4Te2","executionInfo":{"status":"ok","timestamp":1716713358136,"user_tz":-120,"elapsed":9,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"7eae9626-6a13-4b1d-852a-6083d976354c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Shareddrives/NLP_Lab_3\n"]}]},{"cell_type":"code","source":["!pip install datasets evaluate -q"],"metadata":{"id":"jPmSLd62UFDZ","executionInfo":{"status":"ok","timestamp":1716713373660,"user_tz":-120,"elapsed":15530,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d36fcd4d-375a-43ef-f070-66b4f8e98f44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["! pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CeG15sR5VdVL","executionInfo":{"status":"ok","timestamp":1716713426104,"user_tz":-120,"elapsed":52465,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"3a31ff61-f144-4b79-b22a-fdce74fb87dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-juoe4k0w\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-juoe4k0w\n","  Resolved https://github.com/huggingface/transformers to commit bdb9106f247fca48a71eb384be25dbbd29b065a8\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (2024.2.2)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.42.0.dev0-py3-none-any.whl size=9128517 sha256=6f70090baacdc7e3b8ca5716cd33e7fc4a9680a967aa88bd8bf1132482093738\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5yq42j3u/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n","Successfully built transformers\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.0\n","    Uninstalling transformers-4.41.0:\n","      Successfully uninstalled transformers-4.41.0\n","Successfully installed transformers-4.42.0.dev0\n"]}]},{"cell_type":"code","source":["! pip install accelerate -U"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_wNep0RVgmH","executionInfo":{"status":"ok","timestamp":1716713510762,"user_tz":-120,"elapsed":84705,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"3bc02232-ec95-42fe-8850-5519e7e31407"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"markdown","source":["We will work with the goemotions dataset for sentiment analysis task:\n","\n","https://github.com/google-research/google-research/tree/master/goemotions"],"metadata":{"id":"-rjstFOAdI8G"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"data/train.tsv\", sep = \"\\t\", names = [\"text\", \"labels\", \"id\"])\n","df[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"oFo6W7TKaOrm","executionInfo":{"status":"ok","timestamp":1716713512389,"user_tz":-120,"elapsed":1655,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"f85b919b-d62e-4bdf-8c92-6bf7867175cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 text labels       id\n","0   My favourite food is anything I didn't have to...     27  eebbqej\n","1   Now if he does off himself, everyone will thin...     27  ed00q6i\n","2                      WHY THE FUCK IS BAYLESS ISOING      2  eezlygj\n","3                         To make her feel threatened     14  ed7ypvh\n","4                              Dirty Southern Wankers      3  ed0bdzj\n","5   OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...     26  edvnz26\n","6   Yes I heard abt the f bombs! That has to be wh...     15  ee3b6wu\n","7   We need more boards and to create a bit more s...   8,20  ef4qmod\n","8   Damn youtube and outrage drama is super lucrat...      0  ed8wbdn\n","9   It might be linked to the trust factor of your...     27  eczgv1o\n","10  Demographics? I don’t know anybody under 35 wh...      6  eel6g5h\n","11  Aww... she'll probably come around eventually,...    1,4  edex4ki\n","12  Hello everyone. Im from Toronto as well. Can c...     27  ef83m1s\n","13  R/sleeptrain Might be time for some sleep trai...      5  efh7xnk\n","14  [NAME] - same fucking problem, slightly better...      3  efdqav7\n","15  Shit, I guess I accidentally bought a Pay-Per-...   3,12  edivtm3\n","16                                   Thank you friend     15  eeqd04y\n","17                                    Fucking coward.      2  edk0z9k\n","18                that is what retardation looks like     27  eeb9aft\n","19  Maybe that’s what happened to the great white ...   6,22  eczq8zg"],"text/html":["\n","  <div id=\"df-6a60d8a2-11e0-4884-a9b2-046d1c93b100\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>My favourite food is anything I didn't have to...</td>\n","      <td>27</td>\n","      <td>eebbqej</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Now if he does off himself, everyone will thin...</td>\n","      <td>27</td>\n","      <td>ed00q6i</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n","      <td>2</td>\n","      <td>eezlygj</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>To make her feel threatened</td>\n","      <td>14</td>\n","      <td>ed7ypvh</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Dirty Southern Wankers</td>\n","      <td>3</td>\n","      <td>ed0bdzj</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n","      <td>26</td>\n","      <td>edvnz26</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n","      <td>15</td>\n","      <td>ee3b6wu</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>We need more boards and to create a bit more s...</td>\n","      <td>8,20</td>\n","      <td>ef4qmod</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Damn youtube and outrage drama is super lucrat...</td>\n","      <td>0</td>\n","      <td>ed8wbdn</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>It might be linked to the trust factor of your...</td>\n","      <td>27</td>\n","      <td>eczgv1o</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Demographics? I don’t know anybody under 35 wh...</td>\n","      <td>6</td>\n","      <td>eel6g5h</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Aww... she'll probably come around eventually,...</td>\n","      <td>1,4</td>\n","      <td>edex4ki</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Hello everyone. Im from Toronto as well. Can c...</td>\n","      <td>27</td>\n","      <td>ef83m1s</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>R/sleeptrain Might be time for some sleep trai...</td>\n","      <td>5</td>\n","      <td>efh7xnk</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>[NAME] - same fucking problem, slightly better...</td>\n","      <td>3</td>\n","      <td>efdqav7</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Shit, I guess I accidentally bought a Pay-Per-...</td>\n","      <td>3,12</td>\n","      <td>edivtm3</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Thank you friend</td>\n","      <td>15</td>\n","      <td>eeqd04y</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Fucking coward.</td>\n","      <td>2</td>\n","      <td>edk0z9k</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>that is what retardation looks like</td>\n","      <td>27</td>\n","      <td>eeb9aft</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Maybe that’s what happened to the great white ...</td>\n","      <td>6,22</td>\n","      <td>eczq8zg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a60d8a2-11e0-4884-a9b2-046d1c93b100')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6a60d8a2-11e0-4884-a9b2-046d1c93b100 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6a60d8a2-11e0-4884-a9b2-046d1c93b100');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-37710ddb-df76-4760-8d38-7725c978d41c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37710ddb-df76-4760-8d38-7725c978d41c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-37710ddb-df76-4760-8d38-7725c978d41c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df[:20]\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"My favourite food is anything I didn't have to cook myself.\",\n          \"Fucking coward.\",\n          \"Shit, I guess I accidentally bought a Pay-Per-View boxing match\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"3,12\",\n          \"1,4\",\n          \"27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"eebbqej\",\n          \"edk0z9k\",\n          \"edivtm3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["For a single-label classification, we select the first class for every example and save the result into a comma-separated CSV file\n","\n","For a multi-label classification task: the dataset shall be converted into the JSON format, where every example goes in a separate line"],"metadata":{"id":"fWWbJwYoaUZc"}},{"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","\n","training_labels = set()\n","neutral_class = \"27\"\n","\n","for set_type in [\"train\", \"dev\", \"test\"]:\n","    df = pd.read_csv(\"data/%s.tsv\" % (set_type), sep = \"\\t\", names = [\"text\", \"labels\", \"id\"])\n","    df[\"single_label\"] = df[\"labels\"].apply(lambda x: x.split(\",\")[0])\n","    df[[\"text\", \"single_label\"]].to_csv(\"data/%s_single_label.csv\" % (set_type), index=False)\n","\n","    with open(\"data/%s.json\" % (set_type), \"w\") as fout:\n","        for nrow,row in df.iterrows():\n","            example = {}\n","            example[\"text\"] = row[\"text\"]\n","            example[\"labels\"] = row[\"labels\"].split(\",\")\n","            if set_type!=\"train\":\n","                example[\"labels\"] = [label if label in training_labels else neutral_class for label in example[\"labels\"]]\n","            json.dump(example, fout)\n","            fout.write(\"\\n\")\n","            if set_type == \"train\":\n","                training_labels.update(example[\"labels\"])"],"metadata":{"id":"hW9n8wUpd8uF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download run_classification.py into your folder with this notebook from:\n","\n","https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification"],"metadata":{"id":"jnZZYa2ZWhqM"}},{"cell_type":"markdown","source":["Run training using GPU (Runtime -> Change runtime type -> T4 GPU)"],"metadata":{"id":"-FZnxIWnWQpl"}},{"cell_type":"markdown","source":["# 3 EPOCHS - 32 BATCH SIZE - 128 MAX SEQ LENGHT - 2e-5 LEARNING RATE"],"metadata":{"id":"b8IOXytCBJdu"}},{"cell_type":"code","source":["# For single-label classifiction, we provide .csv files\n","#! python run_classification.py --model_name_or_path  bert-large-uncased --train_file \"data/train_single_label.csv\" --validation_file \"data/dev_single_label.csv\" --test_file \"data/test_single_label.csv\" --shuffle_train_dataset --metric_name f1 --text_column_name \"text\" --label_column_name \"single_label\" --do_train --do_eval --do_predict --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 5 --output_dir bert_large_uncased_goemotions_single_label"],"metadata":{"id":"Kd1zUKrhTHPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For multi-label classifiction, we provide .json files\n","!python run_classification.py --model_name_or_path  bert-large-uncased --train_file \"data/train.json\" --validation_file \"data/dev.json\" --test_file \"data/test.json\" --shuffle_train_dataset --metric_name f1 --text_column_name \"text\" --label_column_name \"labels\" --do_train --do_eval --do_predict --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir bert_large_uncased_goemotions"],"metadata":{"id":"8doM-heTdgm0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716141462774,"user_tz":-120,"elapsed":8628497,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"35ed22be-75f9-41c9-e50a-9a830b2ad167"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-19 15:33:54.455511: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-19 15:33:54.455577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-19 15:33:54.456991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-19 15:33:54.464224: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-19 15:33:55.755730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/19/2024 15:34:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/19/2024 15:34:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_steps=None,\n","eval_strategy=no,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=bert_large_uncased_goemotions/runs/May19_15-34-00_1697bdd1cb65,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=bert_large_uncased_goemotions,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=32,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=bert_large_uncased_goemotions,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","05/19/2024 15:34:00 - INFO - __main__ - load a local file for train: data/train.json\n","05/19/2024 15:34:00 - INFO - __main__ - load a local file for validation: data/dev.json\n","05/19/2024 15:34:00 - INFO - __main__ - load a local file for test: data/test.json\n","Using custom data configuration default-2fd72d8d7208eb78\n","05/19/2024 15:34:01 - INFO - datasets.builder - Using custom data configuration default-2fd72d8d7208eb78\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/19/2024 15:34:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","05/19/2024 15:34:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","05/19/2024 15:34:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","05/19/2024 15:34:01 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","05/19/2024 15:34:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","05/19/2024 15:34:01 - INFO - __main__ - Label type is list, doing multi-label classification\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|configuration_utils.py:733] 2024-05-19 15:34:02,231 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:796] 2024-05-19 15:34:02,239 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","05/19/2024 15:34:02 - INFO - __main__ - setting problem type to multi label classification\n","[INFO|configuration_utils.py:733] 2024-05-19 15:34:02,496 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:796] 2024-05-19 15:34:02,497 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_utils_base.py:2108] 2024-05-19 15:34:02,499 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/vocab.txt\n","[INFO|tokenization_utils_base.py:2108] 2024-05-19 15:34:02,499 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer.json\n","[INFO|tokenization_utils_base.py:2108] 2024-05-19 15:34:02,499 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2108] 2024-05-19 15:34:02,499 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2108] 2024-05-19 15:34:02,499 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer_config.json\n","[INFO|configuration_utils.py:733] 2024-05-19 15:34:02,499 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:796] 2024-05-19 15:34:02,500 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|modeling_utils.py:3474] 2024-05-19 15:34:02,667 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/model.safetensors\n","[INFO|modeling_utils.py:4270] 2024-05-19 15:34:08,208 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:4282] 2024-05-19 15:34:08,208 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/19/2024 15:34:08 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-56e363575eba6df9.arrow\n","05/19/2024 15:34:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-56e363575eba6df9.arrow\n","Running tokenizer on dataset:   0% 0/5426 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2762dc70250b3eca.arrow\n","05/19/2024 15:34:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2762dc70250b3eca.arrow\n","Running tokenizer on dataset: 100% 5426/5426 [00:00<00:00, 6584.25 examples/s]\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9f8cd0b6cfd146e3.arrow\n","05/19/2024 15:34:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9f8cd0b6cfd146e3.arrow\n","05/19/2024 15:34:09 - INFO - __main__ - Shuffling the training dataset\n","Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-1adad1c6601b5cad.arrow\n","05/19/2024 15:34:09 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-2fd72d8d7208eb78/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-1adad1c6601b5cad.arrow\n","05/19/2024 15:34:09 - INFO - __main__ - Sample 41905 of the training set: {'text': \"I'm sensitive! :'(\", 'label': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': \"I'm sensitive! :'(\", 'input_ids': [101, 1045, 1005, 1049, 7591, 999, 1024, 1005, 1006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/19/2024 15:34:09 - INFO - __main__ - Sample 7296 of the training set: {'text': 'I see, good one.', 'label': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'I see, good one.', 'input_ids': [101, 1045, 2156, 1010, 2204, 2028, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/19/2024 15:34:09 - INFO - __main__ - Sample 1639 of the training set: {'text': 'And I will laugh at everyone wasting their first round pick on him.', 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'And I will laugh at everyone wasting their first round pick on him.', 'input_ids': [101, 1998, 1045, 2097, 4756, 2012, 3071, 18313, 2037, 2034, 2461, 4060, 2006, 2032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/19/2024 15:34:10 - INFO - __main__ - Using metric f1 for evaluation.\n","[INFO|trainer.py:804] 2024-05-19 15:34:11,274 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2078] 2024-05-19 15:34:11,292 >> ***** Running training *****\n","[INFO|trainer.py:2079] 2024-05-19 15:34:11,292 >>   Num examples = 43,410\n","[INFO|trainer.py:2080] 2024-05-19 15:34:11,292 >>   Num Epochs = 3\n","[INFO|trainer.py:2081] 2024-05-19 15:34:11,292 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:2084] 2024-05-19 15:34:11,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:2085] 2024-05-19 15:34:11,292 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2086] 2024-05-19 15:34:11,292 >>   Total optimization steps = 4,071\n","[INFO|trainer.py:2087] 2024-05-19 15:34:11,293 >>   Number of trainable parameters = 335,170,588\n","{'loss': 0.1584, 'grad_norm': 0.3092053532600403, 'learning_rate': 1.7543601080815527e-05, 'epoch': 0.37}\n"," 12% 500/4071 [16:37<1:58:50,  2.00s/it][INFO|trainer.py:3410] 2024-05-19 15:50:49,061 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-500\n","[INFO|configuration_utils.py:472] 2024-05-19 15:50:49,070 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-500/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 15:50:57,227 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 15:50:57,930 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 15:50:57,941 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-500/special_tokens_map.json\n","{'loss': 0.1013, 'grad_norm': 0.47909900546073914, 'learning_rate': 1.508720216163105e-05, 'epoch': 0.74}\n"," 25% 1000/4071 [33:37<1:42:12,  2.00s/it][INFO|trainer.py:3410] 2024-05-19 16:07:48,377 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-1000\n","[INFO|configuration_utils.py:472] 2024-05-19 16:07:48,386 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 16:07:54,999 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 16:07:55,005 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 16:07:55,014 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.091, 'grad_norm': 0.32648560404777527, 'learning_rate': 1.2630803242446574e-05, 'epoch': 1.11}\n"," 37% 1500/4071 [50:37<1:25:44,  2.00s/it][INFO|trainer.py:3410] 2024-05-19 16:24:48,610 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-1500\n","[INFO|configuration_utils.py:472] 2024-05-19 16:24:48,619 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 16:24:56,735 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 16:24:56,744 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 16:24:56,754 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.0829, 'grad_norm': 0.32608887553215027, 'learning_rate': 1.01744043232621e-05, 'epoch': 1.47}\n"," 49% 2000/4071 [1:07:44<1:08:49,  1.99s/it][INFO|trainer.py:3410] 2024-05-19 16:41:55,839 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-2000\n","[INFO|configuration_utils.py:472] 2024-05-19 16:41:55,846 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 16:42:03,768 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 16:42:03,777 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 16:42:03,783 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.0805, 'grad_norm': 0.673120379447937, 'learning_rate': 7.718005404077623e-06, 'epoch': 1.84}\n"," 61% 2500/4071 [1:24:52<52:11,  1.99s/it][INFO|trainer.py:3410] 2024-05-19 16:59:03,590 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-2500\n","[INFO|configuration_utils.py:472] 2024-05-19 16:59:03,598 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 16:59:10,123 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 16:59:10,137 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 16:59:10,144 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.0744, 'grad_norm': 0.3586982488632202, 'learning_rate': 5.261606484893147e-06, 'epoch': 2.21}\n"," 74% 3000/4071 [1:41:52<35:45,  2.00s/it][INFO|trainer.py:3410] 2024-05-19 17:16:03,768 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-3000\n","[INFO|configuration_utils.py:472] 2024-05-19 17:16:03,777 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 17:16:13,550 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 17:16:13,558 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 17:16:13,564 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.0688, 'grad_norm': 0.4127521514892578, 'learning_rate': 2.8052075657086715e-06, 'epoch': 2.58}\n"," 86% 3500/4071 [1:58:54<19:06,  2.01s/it][INFO|trainer.py:3410] 2024-05-19 17:33:05,626 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-3500\n","[INFO|configuration_utils.py:472] 2024-05-19 17:33:05,637 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 17:33:20,249 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 17:33:20,259 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 17:33:20,264 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.0686, 'grad_norm': 0.2592448890209198, 'learning_rate': 3.488086465241956e-07, 'epoch': 2.95}\n"," 98% 4000/4071 [2:16:07<02:22,  2.00s/it][INFO|trainer.py:3410] 2024-05-19 17:50:18,619 >> Saving model checkpoint to bert_large_uncased_goemotions/checkpoint-4000\n","[INFO|configuration_utils.py:472] 2024-05-19 17:50:18,627 >> Configuration saved in bert_large_uncased_goemotions/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 17:50:25,603 >> Model weights saved in bert_large_uncased_goemotions/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 17:50:26,128 >> tokenizer config file saved in bert_large_uncased_goemotions/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 17:50:26,144 >> Special tokens file saved in bert_large_uncased_goemotions/checkpoint-4000/special_tokens_map.json\n","100% 4071/4071 [2:18:52<00:00,  1.76s/it][INFO|trainer.py:2329] 2024-05-19 17:53:04,187 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 8333.0001, 'train_samples_per_second': 15.628, 'train_steps_per_second': 0.489, 'train_loss': 0.09030889639014607, 'epoch': 3.0}\n","100% 4071/4071 [2:18:52<00:00,  2.05s/it]\n","[INFO|trainer.py:3410] 2024-05-19 17:53:04,307 >> Saving model checkpoint to bert_large_uncased_goemotions\n","[INFO|configuration_utils.py:472] 2024-05-19 17:53:04,326 >> Configuration saved in bert_large_uncased_goemotions/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-19 17:53:12,949 >> Model weights saved in bert_large_uncased_goemotions/model.safetensors\n","[INFO|tokenization_utils_base.py:2513] 2024-05-19 17:53:12,963 >> tokenizer config file saved in bert_large_uncased_goemotions/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2522] 2024-05-19 17:53:12,975 >> Special tokens file saved in bert_large_uncased_goemotions/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               = 28260071GF\n","  train_loss               =     0.0903\n","  train_runtime            = 2:18:53.00\n","  train_samples            =      43410\n","  train_samples_per_second =     15.628\n","  train_steps_per_second   =      0.489\n","05/19/2024 17:53:13 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:804] 2024-05-19 17:53:13,081 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3719] 2024-05-19 17:53:13,127 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3721] 2024-05-19 17:53:13,127 >>   Num examples = 5426\n","[INFO|trainer.py:3724] 2024-05-19 17:53:13,127 >>   Batch size = 8\n","100% 679/679 [02:11<00:00,  5.15it/s]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_f1                 =      0.587\n","  eval_loss               =     0.0842\n","  eval_runtime            = 0:02:12.15\n","  eval_samples            =       5426\n","  eval_samples_per_second =     41.059\n","  eval_steps_per_second   =      5.138\n","05/19/2024 17:55:25 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:804] 2024-05-19 17:55:25,318 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3719] 2024-05-19 17:55:25,321 >> ***** Running Prediction *****\n","[INFO|trainer.py:3721] 2024-05-19 17:55:25,321 >>   Num examples = 5427\n","[INFO|trainer.py:3724] 2024-05-19 17:55:25,321 >>   Batch size = 8\n","100% 679/679 [02:08<00:00,  5.27it/s]\n","05/19/2024 17:57:34 - INFO - __main__ - ***** Predict results *****\n","05/19/2024 17:57:34 - INFO - __main__ - Predict results saved at bert_large_uncased_goemotions/predict_results.txt\n","[INFO|modelcard.py:450] 2024-05-19 17:57:35,131 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.5869896814715119}]}\n"]}]},{"cell_type":"code","source":["# Evaluation for single-label classification\n","'''\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import re\n","\n","df_test = pd.read_csv(\"data/test_single_label.csv\")\n","df_test_preds = pd.read_csv(\"bert_large_uncased_goemotions_single_label/predict_results.txt\", sep=\"\\t\")\n","true_labels = df_test[\"single_label\"].tolist()\n","pred_labels = df_test_preds[\"prediction\"].tolist()\n","print(classification_report(true_labels, pred_labels))\n","'''"],"metadata":{"id":"UqBOtty-USUN","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1716141494570,"user_tz":-120,"elapsed":567,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"9121f8a0-3173-4b68-b88f-7c272a1aacb6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom sklearn.metrics import classification_report\\nimport pandas as pd\\nimport re\\n\\ndf_test = pd.read_csv(\"data/test_single_label.csv\")\\ndf_test_preds = pd.read_csv(\"bert_large_uncased_goemotions_single_label/predict_results.txt\", sep=\"\\t\")\\ntrue_labels = df_test[\"single_label\"].tolist()\\npred_labels = df_test_preds[\"prediction\"].tolist()\\nprint(classification_report(true_labels, pred_labels))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Evaluation for multi-label classification\n","\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import json\n","import re\n","\n","num_classes = 28\n","true_labels = []\n","pred_labels = []\n","with open(\"data/test.json\", \"r\") as fin:\n","    for line in fin:\n","        multi_hot_vector = [0]*num_classes\n","        labels = json.loads(line)[\"labels\"]\n","        for label in labels:\n","            multi_hot_vector[int(label)] = 1\n","        true_labels.append(multi_hot_vector.copy())\n","\n","df_test_preds = pd.read_csv(\"bert_large_uncased_goemotions/predict_results.txt\", sep=\"\\t\")\n","pred_labels_as_str = df_test_preds[\"prediction\"].tolist()\n","for line in pred_labels_as_str:\n","    multi_hot_vector = [0]*num_classes\n","    labels = json.loads(re.sub(\"'\", \"\\\"\", line))\n","    for label in labels:\n","        multi_hot_vector[int(label)] = 1\n","    pred_labels.append(multi_hot_vector.copy())\n","print(classification_report(true_labels, pred_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjvXYbYdy7AU","executionInfo":{"status":"ok","timestamp":1716461319785,"user_tz":-120,"elapsed":3168,"user":{"displayName":"RAÚL FUENTE ARAMBARRI","userId":"04900094120348270859"}},"outputId":"505fa76a-2f98-4aca-dcc9-7d28689327cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.73      0.72       504\n","           1       0.79      0.85      0.82       264\n","           2       0.62      0.43      0.51       198\n","           3       0.49      0.20      0.28       320\n","           4       0.59      0.32      0.41       351\n","           5       0.58      0.41      0.48       135\n","           6       0.57      0.33      0.42       153\n","           7       0.55      0.49      0.52       284\n","           8       0.66      0.35      0.46        83\n","           9       0.58      0.15      0.23       151\n","          10       0.47      0.34      0.40       267\n","          11       0.65      0.33      0.43       123\n","          12       0.77      0.27      0.40        37\n","          13       0.69      0.30      0.42       103\n","          14       0.70      0.65      0.68        78\n","          15       0.94      0.90      0.92       352\n","          16       0.00      0.00      0.00         6\n","          17       0.72      0.58      0.64       161\n","          18       0.79      0.79      0.79       238\n","          19       1.00      0.04      0.08        23\n","          20       0.66      0.45      0.53       186\n","          21       0.00      0.00      0.00        16\n","          22       0.66      0.13      0.22       145\n","          23       0.00      0.00      0.00        11\n","          24       0.63      0.70      0.66        56\n","          25       0.66      0.49      0.56       156\n","          26       0.65      0.53      0.59       141\n","          27       0.73      0.56      0.64      1787\n","\n","   micro avg       0.70      0.52      0.59      6329\n","   macro avg       0.60      0.40      0.46      6329\n","weighted avg       0.68      0.52      0.57      6329\n"," samples avg       0.58      0.54      0.55      6329\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["Run your fine-tuned model for a single inference:"],"metadata":{"id":"aHl5e8sAUwER"}},{"cell_type":"code","source":["import torch\n","from transformers import pipeline\n","\n","label_names = open(\"data/emotions.txt\", \"r\").read().splitlines()\n","print(label_names)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","pipe = pipeline(\"text-classification\", model=\"bert_large_uncased_goemotions\", return_all_scores=True, device=-1 if device.type==\"cpu\" else 0)\n","#pipe = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-uncased-emotion\", return_all_scores=True, device=-1 if device.type == \"cpu\" else 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_bbJ8O-4fAo","executionInfo":{"status":"ok","timestamp":1716141526402,"user_tz":-120,"elapsed":19723,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"0160b4d2-80af-4078-fce1-48ca3931ceb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n","cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# You may tune this threshold manually or using the development set.\n","# It shows how confident the model can be in the class to take this class for the outcome.\n","# A higher threshold may cause an increase in precision but a drop in recall.\n","# Lower values might get higher recall and lower precision. Some trade-off that increases the overall f1-score can be found.\n","threshold = 0.5\n","\n","#text = \"It's great that you're a recovering addict, that's cool. Have you ever tried DMT?\"\n","text = \"Lol! But I love your last name though. XD\"\n","all_classes_scored = pipe(text)\n","print(all_classes_scored)\n","\n","predictions = []\n","for pred in all_classes_scored[0]:\n","    if pred[\"score\"] > threshold:\n","        predictions.append((pred[\"label\"], pred[\"score\"]))\n","print(predictions)\n","for pred in predictions:\n","    print(label_names[int(pred[0])])\n","    #print(pred[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWMt6NT266j6","executionInfo":{"status":"ok","timestamp":1716141553902,"user_tz":-120,"elapsed":12,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"9cab0383-173b-433a-b5ab-24c9984a9592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[{'label': '0', 'score': 0.03638361021876335}, {'label': '1', 'score': 0.7485623955726624}, {'label': '10', 'score': 0.013902966864407063}, {'label': '11', 'score': 0.005694305524230003}, {'label': '12', 'score': 0.0047791581600904465}, {'label': '13', 'score': 0.008352021686732769}, {'label': '14', 'score': 0.007438547443598509}, {'label': '15', 'score': 0.010110200382769108}, {'label': '16', 'score': 0.0027108625508844852}, {'label': '17', 'score': 0.04487846791744232}, {'label': '18', 'score': 0.7560065388679504}, {'label': '19', 'score': 0.002816347870975733}, {'label': '2', 'score': 0.008922049775719643}, {'label': '20', 'score': 0.011926557868719101}, {'label': '21', 'score': 0.004613838624209166}, {'label': '22', 'score': 0.008905000984668732}, {'label': '23', 'score': 0.005132374819368124}, {'label': '24', 'score': 0.0031983035150915384}, {'label': '25', 'score': 0.010042086243629456}, {'label': '26', 'score': 0.009069588035345078}, {'label': '27', 'score': 0.01455884799361229}, {'label': '3', 'score': 0.008901979774236679}, {'label': '4', 'score': 0.019118258729577065}, {'label': '5', 'score': 0.007184064015746117}, {'label': '6', 'score': 0.013927044346928596}, {'label': '7', 'score': 0.014576194807887077}, {'label': '8', 'score': 0.011394202709197998}, {'label': '9', 'score': 0.005257497075945139}]]\n","[('1', 0.7485623955726624), ('18', 0.7560065388679504)]\n","amusement\n","love\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Uq0qX_Grjbxq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 EPOCH - 32 BATCH SIZE - 128 MAX SEQ LENGHT - 2e-5 LEARNING RATE"],"metadata":{"id":"u2unc2ziBhBg"}},{"cell_type":"code","source":["# For multi-label classifiction, we provide .json files\n","!python run_classification.py --model_name_or_path  bert-large-uncased --train_file \"data/train.json\" --validation_file \"data/dev.json\" --test_file \"data/test.json\" --shuffle_train_dataset --metric_name f1 --text_column_name \"text\" --label_column_name \"labels\" --do_train --do_eval --do_predict --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 1 --output_dir bert_large_uncased_goemotions_1_epoch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716464653636,"user_tz":-120,"elapsed":3298776,"user":{"displayName":"RAÚL FUENTE ARAMBARRI","userId":"04900094120348270859"}},"outputId":"24f499d7-ad75-4e20-f56f-3ff6a61ac8a9","id":"UkGoTwOHBjGc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-23 10:49:18.969710: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-23 10:49:18.969764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-23 10:49:18.973698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-23 10:49:18.981028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-23 10:49:20.124333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/23/2024 10:49:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/23/2024 10:49:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_steps=None,\n","eval_strategy=no,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=bert_large_uncased_goemotions_1_epoch/runs/May23_10-49-24_31243aabbd84,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=bert_large_uncased_goemotions_1_epoch,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=32,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=bert_large_uncased_goemotions_1_epoch,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","05/23/2024 10:49:24 - INFO - __main__ - load a local file for train: data/train.json\n","05/23/2024 10:49:24 - INFO - __main__ - load a local file for validation: data/dev.json\n","05/23/2024 10:49:24 - INFO - __main__ - load a local file for test: data/test.json\n","Using custom data configuration default-f24e4fc62814be0d\n","05/23/2024 10:49:25 - INFO - datasets.builder - Using custom data configuration default-f24e4fc62814be0d\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/23/2024 10:49:25 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","05/23/2024 10:49:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","05/23/2024 10:49:25 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","05/23/2024 10:49:25 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","05/23/2024 10:49:25 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n","05/23/2024 10:49:25 - INFO - __main__ - Label type is list, doing multi-label classification\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|configuration_utils.py:733] 2024-05-23 10:49:26,004 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-23 10:49:26,012 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","05/23/2024 10:49:26 - INFO - __main__ - setting problem type to multi label classification\n","[INFO|configuration_utils.py:733] 2024-05-23 10:49:26,104 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-23 10:49:26,105 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_utils_base.py:2109] 2024-05-23 10:49:26,106 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/vocab.txt\n","[INFO|tokenization_utils_base.py:2109] 2024-05-23 10:49:26,106 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer.json\n","[INFO|tokenization_utils_base.py:2109] 2024-05-23 10:49:26,106 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-23 10:49:26,106 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-23 10:49:26,106 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer_config.json\n","[INFO|configuration_utils.py:733] 2024-05-23 10:49:26,107 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-23 10:49:26,108 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|modeling_utils.py:3474] 2024-05-23 10:49:26,260 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/model.safetensors\n","[INFO|modeling_utils.py:4270] 2024-05-23 10:49:31,410 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:4282] 2024-05-23 10:49:31,410 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/23/2024 10:49:31 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-417fd595bb74c826.arrow\n","05/23/2024 10:49:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-417fd595bb74c826.arrow\n","Running tokenizer on dataset:   0% 0/5426 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-966678765ab07170.arrow\n","05/23/2024 10:49:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-966678765ab07170.arrow\n","Running tokenizer on dataset: 100% 5426/5426 [00:00<00:00, 7366.62 examples/s]\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-64a2ca750859a3de.arrow\n","05/23/2024 10:49:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-64a2ca750859a3de.arrow\n","05/23/2024 10:49:32 - INFO - __main__ - Shuffling the training dataset\n","Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-1182b81245460d3a.arrow\n","05/23/2024 10:49:32 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-f24e4fc62814be0d/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-1182b81245460d3a.arrow\n","05/23/2024 10:49:32 - INFO - __main__ - Sample 41905 of the training set: {'text': \"I'm sensitive! :'(\", 'label': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': \"I'm sensitive! :'(\", 'input_ids': [101, 1045, 1005, 1049, 7591, 999, 1024, 1005, 1006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/23/2024 10:49:32 - INFO - __main__ - Sample 7296 of the training set: {'text': 'I see, good one.', 'label': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'I see, good one.', 'input_ids': [101, 1045, 2156, 1010, 2204, 2028, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/23/2024 10:49:32 - INFO - __main__ - Sample 1639 of the training set: {'text': 'And I will laugh at everyone wasting their first round pick on him.', 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'And I will laugh at everyone wasting their first round pick on him.', 'input_ids': [101, 1998, 1045, 2097, 4756, 2012, 3071, 18313, 2037, 2034, 2461, 4060, 2006, 2032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/23/2024 10:49:33 - INFO - __main__ - Using metric f1 for evaluation.\n","[INFO|trainer.py:805] 2024-05-23 10:49:33,885 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, text. If sentence, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2108] 2024-05-23 10:49:33,903 >> ***** Running training *****\n","[INFO|trainer.py:2109] 2024-05-23 10:49:33,903 >>   Num examples = 43,410\n","[INFO|trainer.py:2110] 2024-05-23 10:49:33,903 >>   Num Epochs = 1\n","[INFO|trainer.py:2111] 2024-05-23 10:49:33,903 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:2114] 2024-05-23 10:49:33,903 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:2115] 2024-05-23 10:49:33,903 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2116] 2024-05-23 10:49:33,903 >>   Total optimization steps = 1,357\n","[INFO|trainer.py:2117] 2024-05-23 10:49:33,905 >>   Number of trainable parameters = 335,170,588\n","{'loss': 0.1597, 'grad_norm': 0.2496412843465805, 'learning_rate': 1.2630803242446574e-05, 'epoch': 0.37}\n"," 37% 500/1357 [17:40<30:33,  2.14s/it][INFO|trainer.py:3444] 2024-05-23 11:07:14,293 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch/checkpoint-500\n","[INFO|configuration_utils.py:472] 2024-05-23 11:07:14,305 >> Configuration saved in bert_large_uncased_goemotions_1_epoch/checkpoint-500/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-23 11:07:30,573 >> Model weights saved in bert_large_uncased_goemotions_1_epoch/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-23 11:07:30,585 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-23 11:07:30,593 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch/checkpoint-500/special_tokens_map.json\n","{'loss': 0.1046, 'grad_norm': 0.3598657250404358, 'learning_rate': 5.261606484893147e-06, 'epoch': 0.74}\n"," 74% 1000/1357 [36:08<12:37,  2.12s/it][INFO|trainer.py:3444] 2024-05-23 11:25:42,420 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch/checkpoint-1000\n","[INFO|configuration_utils.py:472] 2024-05-23 11:25:42,428 >> Configuration saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-23 11:25:51,828 >> Model weights saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-23 11:25:52,114 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-23 11:25:52,121 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1000/special_tokens_map.json\n","100% 1357/1357 [49:09<00:00,  1.88s/it][INFO|trainer.py:3444] 2024-05-23 11:38:43,009 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch/checkpoint-1357\n","[INFO|configuration_utils.py:472] 2024-05-23 11:38:43,018 >> Configuration saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1357/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-23 11:38:53,021 >> Model weights saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1357/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-23 11:38:53,036 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1357/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-23 11:38:53,042 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch/checkpoint-1357/special_tokens_map.json\n","[INFO|trainer.py:2358] 2024-05-23 11:39:11,376 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2977.472, 'train_samples_per_second': 14.579, 'train_steps_per_second': 0.456, 'train_loss': 0.12331914374647372, 'epoch': 1.0}\n","100% 1357/1357 [49:37<00:00,  2.19s/it]\n","[INFO|trainer.py:3444] 2024-05-23 11:39:11,408 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch\n","[INFO|configuration_utils.py:472] 2024-05-23 11:39:11,418 >> Configuration saved in bert_large_uncased_goemotions_1_epoch/config.json\n","[INFO|modeling_utils.py:2618] 2024-05-23 11:39:20,142 >> Model weights saved in bert_large_uncased_goemotions_1_epoch/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-23 11:39:20,317 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-23 11:39:20,329 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        1.0\n","  total_flos               =  9420023GF\n","  train_loss               =     0.1233\n","  train_runtime            = 0:49:37.47\n","  train_samples            =      43410\n","  train_samples_per_second =     14.579\n","  train_steps_per_second   =      0.456\n","05/23/2024 11:39:20 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:805] 2024-05-23 11:39:20,465 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, text. If sentence, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3753] 2024-05-23 11:39:20,492 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3755] 2024-05-23 11:39:20,492 >>   Num examples = 5426\n","[INFO|trainer.py:3758] 2024-05-23 11:39:20,492 >>   Batch size = 8\n","100% 679/679 [02:24<00:00,  4.70it/s]\n","***** eval metrics *****\n","  epoch                   =        1.0\n","  eval_f1                 =     0.5122\n","  eval_loss               =     0.0946\n","  eval_runtime            = 0:02:25.02\n","  eval_samples            =       5426\n","  eval_samples_per_second =     37.413\n","  eval_steps_per_second   =      4.682\n","05/23/2024 11:41:45 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:805] 2024-05-23 11:41:45,579 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, text. If sentence, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3753] 2024-05-23 11:41:45,583 >> ***** Running Prediction *****\n","[INFO|trainer.py:3755] 2024-05-23 11:41:45,584 >>   Num examples = 5427\n","[INFO|trainer.py:3758] 2024-05-23 11:41:45,584 >>   Batch size = 8\n","100% 679/679 [02:22<00:00,  4.78it/s]\n","05/23/2024 11:44:07 - INFO - __main__ - ***** Predict results *****\n","05/23/2024 11:44:07 - INFO - __main__ - Predict results saved at bert_large_uncased_goemotions_1_epoch/predict_results.txt\n","[INFO|modelcard.py:449] 2024-05-23 11:44:08,241 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.5122153209109731}]}\n"]}]},{"cell_type":"code","source":["# Evaluation for multi-label classification\n","\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import json\n","import re\n","\n","num_classes = 28\n","true_labels = []\n","pred_labels = []\n","with open(\"data/test.json\", \"r\") as fin:\n","    for line in fin:\n","        multi_hot_vector = [0]*num_classes\n","        labels = json.loads(line)[\"labels\"]\n","        for label in labels:\n","            multi_hot_vector[int(label)] = 1\n","        true_labels.append(multi_hot_vector.copy())\n","\n","df_test_preds = pd.read_csv(\"bert_large_uncased_goemotions_1_epoch/predict_results.txt\", sep=\"\\t\")\n","pred_labels_as_str = df_test_preds[\"prediction\"].tolist()\n","for line in pred_labels_as_str:\n","    multi_hot_vector = [0]*num_classes\n","    labels = json.loads(re.sub(\"'\", \"\\\"\", line))\n","    for label in labels:\n","        multi_hot_vector[int(label)] = 1\n","    pred_labels.append(multi_hot_vector.copy())\n","print(classification_report(true_labels, pred_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716464663844,"user_tz":-120,"elapsed":3033,"user":{"displayName":"RAÚL FUENTE ARAMBARRI","userId":"04900094120348270859"}},"outputId":"06e5c2bb-2afc-41db-85e9-46ae537678d2","id":"DBvEDluTBjGd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.61      0.66       504\n","           1       0.80      0.85      0.82       264\n","           2       0.69      0.15      0.24       198\n","           3       0.00      0.00      0.00       320\n","           4       0.74      0.14      0.24       351\n","           5       0.00      0.00      0.00       135\n","           6       0.46      0.04      0.07       153\n","           7       0.56      0.35      0.43       284\n","           8       0.00      0.00      0.00        83\n","           9       0.00      0.00      0.00       151\n","          10       0.68      0.06      0.12       267\n","          11       0.00      0.00      0.00       123\n","          12       0.00      0.00      0.00        37\n","          13       0.00      0.00      0.00       103\n","          14       1.00      0.05      0.10        78\n","          15       0.95      0.89      0.92       352\n","          16       0.00      0.00      0.00         6\n","          17       0.74      0.30      0.42       161\n","          18       0.80      0.83      0.81       238\n","          19       0.00      0.00      0.00        23\n","          20       0.76      0.40      0.52       186\n","          21       0.00      0.00      0.00        16\n","          22       0.00      0.00      0.00       145\n","          23       0.00      0.00      0.00        11\n","          24       0.56      0.18      0.27        56\n","          25       0.80      0.33      0.47       156\n","          26       0.68      0.30      0.41       141\n","          27       0.74      0.55      0.63      1787\n","\n","   micro avg       0.76      0.39      0.51      6329\n","   macro avg       0.42      0.22      0.26      6329\n","weighted avg       0.60      0.39      0.44      6329\n"," samples avg       0.45      0.41      0.42      6329\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["# 1 EPOCH - 16 BATCH SIZE - 128 MAX SEQ LENGHT - 3e-5 LEARNING RATE - BETTER"],"metadata":{"id":"PeaIHk8TH2wa"}},{"cell_type":"code","source":["#i get the CUDA out of memory error typically occurs when your GPU does not have enough memory to handle the batch size you have specified -> batch size 64 cannot\n","!python run_classification.py --model_name_or_path  bert-large-uncased --train_file \"data/train.json\" --validation_file \"data/dev.json\" --test_file \"data/test.json\" --shuffle_train_dataset --metric_name f1 --text_column_name \"text\" --label_column_name \"labels\" --do_train --do_eval --do_predict --max_seq_length 128 --per_device_train_batch_size 16 --learning_rate 3e-5 --num_train_epochs 1 --output_dir bert_large_uncased_goemotions_1_epoch_cambioos"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5db5880c-a7c7-48eb-ffd1-16b94f8603ec","id":"ytVlbUMvH2wb","executionInfo":{"status":"ok","timestamp":1716663071528,"user_tz":-120,"elapsed":242276,"user":{"displayName":"RAÚL FUENTE ARAMBARRI","userId":"04900094120348270859"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-25 18:47:20.366143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-25 18:47:20.366220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-25 18:47:20.451048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-25 18:47:20.463196: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-25 18:47:22.228609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/25/2024 18:47:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/25/2024 18:47:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_steps=None,\n","eval_strategy=no,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=bert_large_uncased_goemotions_1_epoch_cambioos/runs/May25_18-47-26_d0082a448e6a,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=bert_large_uncased_goemotions_1_epoch_cambioos,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=bert_large_uncased_goemotions_1_epoch_cambioos,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","05/25/2024 18:47:26 - INFO - __main__ - load a local file for train: data/train.json\n","05/25/2024 18:47:26 - INFO - __main__ - load a local file for validation: data/dev.json\n","05/25/2024 18:47:26 - INFO - __main__ - load a local file for test: data/test.json\n","Using custom data configuration default-46b394acd8f6f3d0\n","05/25/2024 18:47:27 - INFO - datasets.builder - Using custom data configuration default-46b394acd8f6f3d0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/25/2024 18:47:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","05/25/2024 18:47:27 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n","05/25/2024 18:47:27 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n","Downloading took 0.0 min\n","05/25/2024 18:47:27 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","05/25/2024 18:47:27 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","05/25/2024 18:47:27 - INFO - datasets.builder - Generating train split\n","Generating train split: 43410 examples [00:00, 697842.32 examples/s]\n","Generating validation split\n","05/25/2024 18:47:27 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 5426 examples [00:00, 518258.68 examples/s]\n","Generating test split\n","05/25/2024 18:47:27 - INFO - datasets.builder - Generating test split\n","Generating test split: 5427 examples [00:00, 555955.54 examples/s]\n","Unable to verify splits sizes.\n","05/25/2024 18:47:27 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n","05/25/2024 18:47:27 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n","05/25/2024 18:47:27 - INFO - __main__ - Label type is list, doing multi-label classification\n","config.json: 100% 571/571 [00:00<00:00, 2.77MB/s]\n","[INFO|configuration_utils.py:733] 2024-05-25 18:47:28,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-25 18:47:28,970 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","05/25/2024 18:47:28 - INFO - __main__ - setting problem type to multi label classification\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 306kB/s]\n","[INFO|configuration_utils.py:733] 2024-05-25 18:47:29,455 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-25 18:47:29,456 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 532kB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 720kB/s]\n","[INFO|tokenization_utils_base.py:2109] 2024-05-25 18:47:32,103 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/vocab.txt\n","[INFO|tokenization_utils_base.py:2109] 2024-05-25 18:47:32,103 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer.json\n","[INFO|tokenization_utils_base.py:2109] 2024-05-25 18:47:32,104 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-25 18:47:32,104 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-25 18:47:32,104 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer_config.json\n","[INFO|configuration_utils.py:733] 2024-05-25 18:47:32,104 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-25 18:47:32,105 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","model.safetensors: 100% 1.34G/1.34G [01:16<00:00, 17.7MB/s]\n","[INFO|modeling_utils.py:3477] 2024-05-25 18:48:49,818 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/model.safetensors\n","[INFO|modeling_utils.py:4273] 2024-05-25 18:48:51,599 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:4285] 2024-05-25 18:48:51,599 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/25/2024 18:48:51 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.\n","Running tokenizer on dataset:   0% 0/43410 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-11092d8ca02e0c5f.arrow\n","05/25/2024 18:48:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-11092d8ca02e0c5f.arrow\n","Running tokenizer on dataset: 100% 43410/43410 [00:07<00:00, 6042.75 examples/s]\n","Running tokenizer on dataset:   0% 0/5426 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-fe554942ee63c871.arrow\n","05/25/2024 18:48:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-fe554942ee63c871.arrow\n","Running tokenizer on dataset: 100% 5426/5426 [00:00<00:00, 7123.71 examples/s]\n","Running tokenizer on dataset:   0% 0/5427 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-7eb139d0cc00432c.arrow\n","05/25/2024 18:48:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-7eb139d0cc00432c.arrow\n","Running tokenizer on dataset: 100% 5427/5427 [00:01<00:00, 5197.01 examples/s]\n","05/25/2024 18:49:00 - INFO - __main__ - Shuffling the training dataset\n","Caching indices mapping at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-b56e4c7908dcfed5.arrow\n","05/25/2024 18:49:00 - INFO - datasets.arrow_dataset - Caching indices mapping at /root/.cache/huggingface/datasets/json/default-46b394acd8f6f3d0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-b56e4c7908dcfed5.arrow\n","05/25/2024 18:49:00 - INFO - __main__ - Sample 41905 of the training set: {'text': \"I'm sensitive! :'(\", 'label': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': \"I'm sensitive! :'(\", 'input_ids': [101, 1045, 1005, 1049, 7591, 999, 1024, 1005, 1006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/25/2024 18:49:00 - INFO - __main__ - Sample 7296 of the training set: {'text': 'I see, good one.', 'label': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'I see, good one.', 'input_ids': [101, 1045, 2156, 1010, 2204, 2028, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/25/2024 18:49:00 - INFO - __main__ - Sample 1639 of the training set: {'text': 'And I will laugh at everyone wasting their first round pick on him.', 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'And I will laugh at everyone wasting their first round pick on him.', 'input_ids': [101, 1998, 1045, 2097, 4756, 2012, 3071, 18313, 2037, 2034, 2461, 4060, 2006, 2032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 22.6MB/s]\n","05/25/2024 18:49:02 - INFO - __main__ - Using metric f1 for evaluation.\n","[INFO|trainer.py:805] 2024-05-25 18:49:03,389 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2108] 2024-05-25 18:49:03,409 >> ***** Running training *****\n","[INFO|trainer.py:2109] 2024-05-25 18:49:03,409 >>   Num examples = 43,410\n","[INFO|trainer.py:2110] 2024-05-25 18:49:03,409 >>   Num Epochs = 1\n","[INFO|trainer.py:2111] 2024-05-25 18:49:03,409 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:2114] 2024-05-25 18:49:03,409 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2115] 2024-05-25 18:49:03,409 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2116] 2024-05-25 18:49:03,409 >>   Total optimization steps = 2,714\n","[INFO|trainer.py:2117] 2024-05-25 18:49:03,411 >>   Number of trainable parameters = 335,170,588\n","  4% 109/2714 [02:03<50:44,  1.17s/it]Traceback (most recent call last):\n","  File \"/content/gdrive/Shareddrives/NLP_Lab_3/run_classification.py\", line 763, in <module>\n","    main()\n","  File \"/content/gdrive/Shareddrives/NLP_Lab_3/run_classification.py\", line 698, in main\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1912, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2250, in _inner_training_loop\n","    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n","KeyboardInterrupt\n","  4% 109/2714 [02:05<49:55,  1.15s/it]\n"]}]},{"cell_type":"code","source":["# Evaluation for multi-label classification\n","\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import json\n","import re\n","\n","num_classes = 28\n","true_labels = []\n","pred_labels = []\n","with open(\"data/test.json\", \"r\") as fin:\n","    for line in fin:\n","        multi_hot_vector = [0]*num_classes\n","        labels = json.loads(line)[\"labels\"]\n","        for label in labels:\n","            multi_hot_vector[int(label)] = 1\n","        true_labels.append(multi_hot_vector.copy())\n","\n","df_test_preds = pd.read_csv(\"bert_large_uncased_goemotions_1_epoch_cambioos/predict_results.txt\", sep=\"\\t\")\n","pred_labels_as_str = df_test_preds[\"prediction\"].tolist()\n","for line in pred_labels_as_str:\n","    multi_hot_vector = [0]*num_classes\n","    labels = json.loads(re.sub(\"'\", \"\\\"\", line))\n","    for label in labels:\n","        multi_hot_vector[int(label)] = 1\n","    pred_labels.append(multi_hot_vector.copy())\n","print(classification_report(true_labels, pred_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716542665324,"user_tz":-120,"elapsed":3269,"user":{"displayName":"RAÚL FUENTE ARAMBARRI","userId":"04900094120348270859"}},"outputId":"dee07a0d-2620-438e-db7d-014b206b911e","id":"uWv1Tmq1H2wb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.68      0.69       504\n","           1       0.79      0.88      0.83       264\n","           2       0.66      0.32      0.43       198\n","           3       0.58      0.11      0.18       320\n","           4       0.70      0.22      0.34       351\n","           5       0.70      0.17      0.27       135\n","           6       0.65      0.26      0.37       153\n","           7       0.59      0.39      0.47       284\n","           8       0.65      0.31      0.42        83\n","           9       0.00      0.00      0.00       151\n","          10       0.52      0.18      0.26       267\n","          11       0.67      0.24      0.35       123\n","          12       1.00      0.05      0.10        37\n","          13       0.85      0.21      0.34       103\n","          14       0.65      0.65      0.65        78\n","          15       0.96      0.87      0.91       352\n","          16       0.00      0.00      0.00         6\n","          17       0.71      0.53      0.60       161\n","          18       0.80      0.83      0.81       238\n","          19       0.00      0.00      0.00        23\n","          20       0.70      0.43      0.53       186\n","          21       0.00      0.00      0.00        16\n","          22       1.00      0.05      0.09       145\n","          23       0.00      0.00      0.00        11\n","          24       0.61      0.66      0.63        56\n","          25       0.65      0.46      0.53       156\n","          26       0.62      0.50      0.55       141\n","          27       0.75      0.55      0.63      1787\n","\n","   micro avg       0.73      0.46      0.57      6329\n","   macro avg       0.59      0.34      0.39      6329\n","weighted avg       0.70      0.46      0.53      6329\n"," samples avg       0.53      0.49      0.50      6329\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["# 1 EPOCH - 16 BATCH SIZE - 64 MAX SEQ LENGHT - 3e-5 LEARNING RATE - SIMILAR"],"metadata":{"id":"GV5GqsgQIjrl"}},{"cell_type":"code","source":["# For multi-label classifiction, we provide .json files\n","!python run_classification.py --model_name_or_path  bert-large-uncased --train_file \"data/train.json\" --validation_file \"data/dev.json\" --test_file \"data/test.json\" --shuffle_train_dataset --metric_name f1 --text_column_name \"text\" --label_column_name \"labels\" --do_train --do_eval --do_predict --max_seq_length 64 --per_device_train_batch_size 16 --learning_rate 3e-5 --num_train_epochs 1 --output_dir bert_large_uncased_goemotions_1_epoch_cambios_ult"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"528ea694-17fa-4398-a896-945898a2f816","id":"3Dn4qgfbIjrl","executionInfo":{"status":"ok","timestamp":1716545205393,"user_tz":-120,"elapsed":2229323,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-24 09:29:42.983302: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-24 09:29:42.983352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-24 09:29:43.091462: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-24 09:29:43.118963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-24 09:29:44.168096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/24/2024 09:29:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/24/2024 09:29:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_steps=None,\n","eval_strategy=no,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=bert_large_uncased_goemotions_1_epoch_cambios_ult/runs/May24_09-29-49_550c79e54ca8,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=bert_large_uncased_goemotions_1_epoch_cambios_ult,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=bert_large_uncased_goemotions_1_epoch_cambios_ult,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","05/24/2024 09:29:49 - INFO - __main__ - load a local file for train: data/train.json\n","05/24/2024 09:29:49 - INFO - __main__ - load a local file for validation: data/dev.json\n","05/24/2024 09:29:49 - INFO - __main__ - load a local file for test: data/test.json\n","Using custom data configuration default-b275ab8d7be306a0\n","05/24/2024 09:29:49 - INFO - datasets.builder - Using custom data configuration default-b275ab8d7be306a0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/24/2024 09:29:49 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","05/24/2024 09:29:49 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n","05/24/2024 09:29:49 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n","Downloading took 0.0 min\n","05/24/2024 09:29:49 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","05/24/2024 09:29:49 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","05/24/2024 09:29:49 - INFO - datasets.builder - Generating train split\n","Generating train split: 43410 examples [00:00, 761274.30 examples/s]\n","Generating validation split\n","05/24/2024 09:29:49 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 5426 examples [00:00, 542626.39 examples/s]\n","Generating test split\n","05/24/2024 09:29:49 - INFO - datasets.builder - Generating test split\n","Generating test split: 5427 examples [00:00, 447411.11 examples/s]\n","Unable to verify splits sizes.\n","05/24/2024 09:29:49 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n","05/24/2024 09:29:49 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n","05/24/2024 09:29:49 - INFO - __main__ - Label type is list, doing multi-label classification\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","config.json: 100% 571/571 [00:00<00:00, 3.29MB/s]\n","[INFO|configuration_utils.py:733] 2024-05-24 09:29:50,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-24 09:29:50,322 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","05/24/2024 09:29:50 - INFO - __main__ - setting problem type to multi label classification\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 318kB/s]\n","[INFO|configuration_utils.py:733] 2024-05-24 09:29:50,490 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-24 09:29:50,491 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 2.00MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 2.68MB/s]\n","[INFO|tokenization_utils_base.py:2109] 2024-05-24 09:29:51,395 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/vocab.txt\n","[INFO|tokenization_utils_base.py:2109] 2024-05-24 09:29:51,395 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer.json\n","[INFO|tokenization_utils_base.py:2109] 2024-05-24 09:29:51,395 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-24 09:29:51,395 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-24 09:29:51,395 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer_config.json\n","[INFO|configuration_utils.py:733] 2024-05-24 09:29:51,396 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-24 09:29:51,397 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","model.safetensors: 100% 1.34G/1.34G [00:05<00:00, 266MB/s]\n","[INFO|modeling_utils.py:3477] 2024-05-24 09:29:56,789 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/model.safetensors\n","[INFO|modeling_utils.py:4273] 2024-05-24 09:29:57,941 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:4285] 2024-05-24 09:29:57,941 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/24/2024 09:29:57 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.\n","Running tokenizer on dataset:   0% 0/43410 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-da75d122c2c83a45.arrow\n","05/24/2024 09:29:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-da75d122c2c83a45.arrow\n","Running tokenizer on dataset: 100% 43410/43410 [00:06<00:00, 6746.37 examples/s]\n","Running tokenizer on dataset:   0% 0/5426 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3ed0931d3baeb9be.arrow\n","05/24/2024 09:30:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3ed0931d3baeb9be.arrow\n","Running tokenizer on dataset: 100% 5426/5426 [00:00<00:00, 10472.57 examples/s]\n","Running tokenizer on dataset:   0% 0/5427 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a27919b0a341a4a7.arrow\n","05/24/2024 09:30:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a27919b0a341a4a7.arrow\n","Running tokenizer on dataset: 100% 5427/5427 [00:00<00:00, 7176.65 examples/s]\n","05/24/2024 09:30:05 - INFO - __main__ - Shuffling the training dataset\n","Caching indices mapping at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-134d446411a5292b.arrow\n","05/24/2024 09:30:05 - INFO - datasets.arrow_dataset - Caching indices mapping at /root/.cache/huggingface/datasets/json/default-b275ab8d7be306a0/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-134d446411a5292b.arrow\n","05/24/2024 09:30:05 - INFO - __main__ - Sample 41905 of the training set: {'text': \"I'm sensitive! :'(\", 'label': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': \"I'm sensitive! :'(\", 'input_ids': [101, 1045, 1005, 1049, 7591, 999, 1024, 1005, 1006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/24/2024 09:30:05 - INFO - __main__ - Sample 7296 of the training set: {'text': 'I see, good one.', 'label': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'I see, good one.', 'input_ids': [101, 1045, 2156, 1010, 2204, 2028, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/24/2024 09:30:05 - INFO - __main__ - Sample 1639 of the training set: {'text': 'And I will laugh at everyone wasting their first round pick on him.', 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'And I will laugh at everyone wasting their first round pick on him.', 'input_ids': [101, 1998, 1045, 2097, 4756, 2012, 3071, 18313, 2037, 2034, 2461, 4060, 2006, 2032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 22.7MB/s]\n","05/24/2024 09:30:06 - INFO - __main__ - Using metric f1 for evaluation.\n","[INFO|trainer.py:805] 2024-05-24 09:30:07,298 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2108] 2024-05-24 09:30:07,316 >> ***** Running training *****\n","[INFO|trainer.py:2109] 2024-05-24 09:30:07,316 >>   Num examples = 43,410\n","[INFO|trainer.py:2110] 2024-05-24 09:30:07,316 >>   Num Epochs = 1\n","[INFO|trainer.py:2111] 2024-05-24 09:30:07,316 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:2114] 2024-05-24 09:30:07,316 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2115] 2024-05-24 09:30:07,316 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2116] 2024-05-24 09:30:07,317 >>   Total optimization steps = 2,714\n","[INFO|trainer.py:2117] 2024-05-24 09:30:07,318 >>   Number of trainable parameters = 335,170,588\n","{'loss': 0.1521, 'grad_norm': 0.5223212242126465, 'learning_rate': 2.447310243183493e-05, 'epoch': 0.18}\n"," 18% 500/2714 [05:41<25:24,  1.45it/s][INFO|trainer.py:3444] 2024-05-24 09:35:48,892 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-500\n","[INFO|configuration_utils.py:472] 2024-05-24 09:35:48,900 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 09:35:59,994 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 09:36:00,009 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 09:36:00,021 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-500/special_tokens_map.json\n","{'loss': 0.1057, 'grad_norm': 0.48802873492240906, 'learning_rate': 1.894620486366986e-05, 'epoch': 0.37}\n"," 37% 1000/2714 [11:51<19:41,  1.45it/s][INFO|trainer.py:3444] 2024-05-24 09:41:58,672 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1000\n","[INFO|configuration_utils.py:472] 2024-05-24 09:41:58,681 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 09:42:07,871 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 09:42:07,878 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 09:42:07,893 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.0958, 'grad_norm': 0.5050638914108276, 'learning_rate': 1.341930729550479e-05, 'epoch': 0.55}\n"," 55% 1500/2714 [17:59<13:49,  1.46it/s][INFO|trainer.py:3444] 2024-05-24 09:48:07,114 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1500\n","[INFO|configuration_utils.py:472] 2024-05-24 09:48:07,126 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 09:48:14,949 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 09:48:14,961 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 09:48:14,970 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.0906, 'grad_norm': 0.6197981834411621, 'learning_rate': 7.89240972733972e-06, 'epoch': 0.74}\n"," 74% 2000/2714 [24:12<08:10,  1.46it/s][INFO|trainer.py:3444] 2024-05-24 09:54:20,296 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2000\n","[INFO|configuration_utils.py:472] 2024-05-24 09:54:20,314 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 09:54:28,621 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 09:54:28,632 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 09:54:28,639 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.0883, 'grad_norm': 0.6168670058250427, 'learning_rate': 2.3655121591746504e-06, 'epoch': 0.92}\n"," 92% 2500/2714 [30:28<02:26,  1.46it/s][INFO|trainer.py:3444] 2024-05-24 10:00:35,563 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2500\n","[INFO|configuration_utils.py:472] 2024-05-24 10:00:35,571 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 10:00:48,256 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 10:00:48,271 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 10:00:48,278 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2500/special_tokens_map.json\n","100% 2714/2714 [33:23<00:00,  1.67it/s][INFO|trainer.py:3444] 2024-05-24 10:03:30,879 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2714\n","[INFO|configuration_utils.py:472] 2024-05-24 10:03:30,885 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2714/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 10:03:42,056 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2714/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 10:03:42,070 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2714/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 10:03:42,082 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/checkpoint-2714/special_tokens_map.json\n","[INFO|trainer.py:2358] 2024-05-24 10:03:57,388 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2030.0706, 'train_samples_per_second': 21.383, 'train_steps_per_second': 1.337, 'train_loss': 0.10534434856170227, 'epoch': 1.0}\n","100% 2714/2714 [33:50<00:00,  1.34it/s]\n","[INFO|trainer.py:3444] 2024-05-24 10:03:57,401 >> Saving model checkpoint to bert_large_uncased_goemotions_1_epoch_cambios_ult\n","[INFO|configuration_utils.py:472] 2024-05-24 10:03:57,409 >> Configuration saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-24 10:04:05,289 >> Model weights saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-24 10:04:05,302 >> tokenizer config file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-24 10:04:05,314 >> Special tokens file saved in bert_large_uncased_goemotions_1_epoch_cambios_ult/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        1.0\n","  total_flos               =  4710011GF\n","  train_loss               =     0.1053\n","  train_runtime            = 0:33:50.07\n","  train_samples            =      43410\n","  train_samples_per_second =     21.383\n","  train_steps_per_second   =      1.337\n","05/24/2024 10:04:05 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:805] 2024-05-24 10:04:05,531 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3753] 2024-05-24 10:04:05,554 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3755] 2024-05-24 10:04:05,554 >>   Num examples = 5426\n","[INFO|trainer.py:3758] 2024-05-24 10:04:05,555 >>   Batch size = 8\n","100% 679/679 [01:17<00:00,  8.79it/s]\n","***** eval metrics *****\n","  epoch                   =        1.0\n","  eval_f1                 =     0.5631\n","  eval_loss               =     0.0851\n","  eval_runtime            = 0:01:17.45\n","  eval_samples            =       5426\n","  eval_samples_per_second =     70.052\n","  eval_steps_per_second   =      8.766\n","05/24/2024 10:05:23 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:805] 2024-05-24 10:05:23,052 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, sentence. If text, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3753] 2024-05-24 10:05:23,056 >> ***** Running Prediction *****\n","[INFO|trainer.py:3755] 2024-05-24 10:05:23,056 >>   Num examples = 5427\n","[INFO|trainer.py:3758] 2024-05-24 10:05:23,056 >>   Batch size = 8\n","100% 679/679 [01:15<00:00,  9.05it/s]\n","05/24/2024 10:06:38 - INFO - __main__ - ***** Predict results *****\n","05/24/2024 10:06:38 - INFO - __main__ - Predict results saved at bert_large_uncased_goemotions_1_epoch_cambios_ult/predict_results.txt\n","[INFO|modelcard.py:449] 2024-05-24 10:06:38,581 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.5630600250505828}]}\n"]}]},{"cell_type":"code","source":["# Evaluation for multi-label classification\n","\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import json\n","import re\n","\n","num_classes = 28\n","true_labels = []\n","pred_labels = []\n","with open(\"data/test.json\", \"r\") as fin:\n","    for line in fin:\n","        multi_hot_vector = [0]*num_classes\n","        labels = json.loads(line)[\"labels\"]\n","        for label in labels:\n","            multi_hot_vector[int(label)] = 1\n","        true_labels.append(multi_hot_vector.copy())\n","\n","df_test_preds = pd.read_csv(\"bert_large_uncased_goemotions_1_epoch_cambios_ult/predict_results.txt\", sep=\"\\t\")\n","pred_labels_as_str = df_test_preds[\"prediction\"].tolist()\n","for line in pred_labels_as_str:\n","    multi_hot_vector = [0]*num_classes\n","    labels = json.loads(re.sub(\"'\", \"\\\"\", line))\n","    for label in labels:\n","        multi_hot_vector[int(label)] = 1\n","    pred_labels.append(multi_hot_vector.copy())\n","print(classification_report(true_labels, pred_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716545436990,"user_tz":-120,"elapsed":3477,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"f45b418c-4511-4912-b38c-62cb68e39613","id":"mB1R2xmuIjrm"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.67      0.69       504\n","           1       0.77      0.90      0.83       264\n","           2       0.71      0.36      0.48       198\n","           3       0.75      0.09      0.17       320\n","           4       0.66      0.20      0.31       351\n","           5       0.74      0.19      0.30       135\n","           6       0.60      0.27      0.38       153\n","           7       0.58      0.44      0.50       284\n","           8       0.73      0.27      0.39        83\n","           9       0.00      0.00      0.00       151\n","          10       0.54      0.16      0.25       267\n","          11       0.89      0.07      0.12       123\n","          12       0.00      0.00      0.00        37\n","          13       0.83      0.19      0.31       103\n","          14       0.70      0.62      0.65        78\n","          15       0.96      0.89      0.92       352\n","          16       0.00      0.00      0.00         6\n","          17       0.70      0.53      0.60       161\n","          18       0.78      0.85      0.81       238\n","          19       0.00      0.00      0.00        23\n","          20       0.70      0.45      0.55       186\n","          21       0.00      0.00      0.00        16\n","          22       0.00      0.00      0.00       145\n","          23       0.00      0.00      0.00        11\n","          24       0.57      0.59      0.58        56\n","          25       0.69      0.47      0.56       156\n","          26       0.64      0.47      0.54       141\n","          27       0.75      0.54      0.63      1787\n","\n","   micro avg       0.74      0.46      0.57      6329\n","   macro avg       0.54      0.33      0.38      6329\n","weighted avg       0.68      0.46      0.52      6329\n"," samples avg       0.52      0.49      0.50      6329\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["# 3 EPOCHS - 16 BATCH SIZE - 128 MAX SEQ LENGHT - 3e-5 LEARNING RATE - TODO"],"metadata":{"id":"2O0bRxt_F4Zd"}},{"cell_type":"code","source":["#i get the CUDA out of memory error typically occurs when your GPU does not have enough memory to handle the batch size you have specified -> batch size 64 cannot\n","!python run_classification.py --model_name_or_path  bert-large-uncased --train_file \"data/train.json\" --validation_file \"data/dev.json\" --test_file \"data/test.json\" --shuffle_train_dataset --metric_name f1 --text_column_name \"text\" --label_column_name \"labels\" --do_train --do_eval --do_predict --max_seq_length 128 --per_device_train_batch_size 16 --learning_rate 3e-5 --num_train_epochs 3 --output_dir bert_large_uncased_goemotions_3_epochs_may_be_better"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac42eb5b-e859-49e1-dd8f-b61818da200f","id":"erfqdAu4F4aG","executionInfo":{"status":"ok","timestamp":1716723752903,"user_tz":-120,"elapsed":10222084,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-26 08:52:20.690278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-26 08:52:20.690344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-26 08:52:20.775391: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-26 08:52:20.794094: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-26 08:52:22.815738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/26/2024 08:52:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/26/2024 08:52:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_steps=None,\n","eval_strategy=no,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=bert_large_uncased_goemotions_3_epochs_may_be_better/runs/May26_08-52-28_58fdf84b8b50,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=bert_large_uncased_goemotions_3_epochs_may_be_better,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=bert_large_uncased_goemotions_3_epochs_may_be_better,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","05/26/2024 08:52:28 - INFO - __main__ - load a local file for train: data/train.json\n","05/26/2024 08:52:28 - INFO - __main__ - load a local file for validation: data/dev.json\n","05/26/2024 08:52:28 - INFO - __main__ - load a local file for test: data/test.json\n","Using custom data configuration default-e21758c3c97cf0c3\n","05/26/2024 08:52:29 - INFO - datasets.builder - Using custom data configuration default-e21758c3c97cf0c3\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/26/2024 08:52:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","05/26/2024 08:52:29 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n","05/26/2024 08:52:29 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n","Downloading took 0.0 min\n","05/26/2024 08:52:29 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","05/26/2024 08:52:29 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","05/26/2024 08:52:29 - INFO - datasets.builder - Generating train split\n","Generating train split: 43410 examples [00:00, 704408.24 examples/s]\n","Generating validation split\n","05/26/2024 08:52:29 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 5426 examples [00:00, 572550.09 examples/s]\n","Generating test split\n","05/26/2024 08:52:29 - INFO - datasets.builder - Generating test split\n","Generating test split: 5427 examples [00:00, 609094.96 examples/s]\n","Unable to verify splits sizes.\n","05/26/2024 08:52:29 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n","05/26/2024 08:52:29 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n","05/26/2024 08:52:29 - INFO - __main__ - Label type is list, doing multi-label classification\n","config.json: 100% 571/571 [00:00<00:00, 4.56MB/s]\n","[INFO|configuration_utils.py:733] 2024-05-26 08:52:29,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-26 08:52:29,957 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","05/26/2024 08:52:29 - INFO - __main__ - setting problem type to multi label classification\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 371kB/s]\n","[INFO|configuration_utils.py:733] 2024-05-26 08:52:30,150 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-26 08:52:30,151 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 6.98MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 13.1MB/s]\n","[INFO|tokenization_utils_base.py:2109] 2024-05-26 08:52:30,598 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/vocab.txt\n","[INFO|tokenization_utils_base.py:2109] 2024-05-26 08:52:30,598 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer.json\n","[INFO|tokenization_utils_base.py:2109] 2024-05-26 08:52:30,598 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-26 08:52:30,598 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2109] 2024-05-26 08:52:30,598 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer_config.json\n","[INFO|configuration_utils.py:733] 2024-05-26 08:52:30,599 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n","[INFO|configuration_utils.py:800] 2024-05-26 08:52:30,599 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-large-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","model.safetensors: 100% 1.34G/1.34G [00:05<00:00, 241MB/s]\n","[INFO|modeling_utils.py:3477] 2024-05-26 08:52:36,482 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/model.safetensors\n","[INFO|modeling_utils.py:4273] 2024-05-26 08:52:37,950 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:4285] 2024-05-26 08:52:37,950 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/26/2024 08:52:38 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.\n","Running tokenizer on dataset:   0% 0/43410 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-000c7be43387387f.arrow\n","05/26/2024 08:52:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-000c7be43387387f.arrow\n","Running tokenizer on dataset: 100% 43410/43410 [00:07<00:00, 6034.71 examples/s]\n","Running tokenizer on dataset:   0% 0/5426 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-c1c69e2672c68619.arrow\n","05/26/2024 08:52:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-c1c69e2672c68619.arrow\n","Running tokenizer on dataset: 100% 5426/5426 [00:00<00:00, 7817.02 examples/s]\n","Running tokenizer on dataset:   0% 0/5427 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-c63bb2e41575479b.arrow\n","05/26/2024 08:52:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-c63bb2e41575479b.arrow\n","Running tokenizer on dataset: 100% 5427/5427 [00:00<00:00, 5632.26 examples/s]\n","05/26/2024 08:52:46 - INFO - __main__ - Shuffling the training dataset\n","Caching indices mapping at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-b527afba7e9ad580.arrow\n","05/26/2024 08:52:46 - INFO - datasets.arrow_dataset - Caching indices mapping at /root/.cache/huggingface/datasets/json/default-e21758c3c97cf0c3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-b527afba7e9ad580.arrow\n","05/26/2024 08:52:46 - INFO - __main__ - Sample 41905 of the training set: {'text': \"I'm sensitive! :'(\", 'label': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': \"I'm sensitive! :'(\", 'input_ids': [101, 1045, 1005, 1049, 7591, 999, 1024, 1005, 1006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/26/2024 08:52:46 - INFO - __main__ - Sample 7296 of the training set: {'text': 'I see, good one.', 'label': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'I see, good one.', 'input_ids': [101, 1045, 2156, 1010, 2204, 2028, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","05/26/2024 08:52:46 - INFO - __main__ - Sample 1639 of the training set: {'text': 'And I will laugh at everyone wasting their first round pick on him.', 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sentence': 'And I will laugh at everyone wasting their first round pick on him.', 'input_ids': [101, 1998, 1045, 2097, 4756, 2012, 3071, 18313, 2037, 2034, 2461, 4060, 2006, 2032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 20.4MB/s]\n","05/26/2024 08:52:47 - INFO - __main__ - Using metric f1 for evaluation.\n","[INFO|trainer.py:805] 2024-05-26 08:52:48,435 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, text. If sentence, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2108] 2024-05-26 08:52:48,454 >> ***** Running training *****\n","[INFO|trainer.py:2109] 2024-05-26 08:52:48,454 >>   Num examples = 43,410\n","[INFO|trainer.py:2110] 2024-05-26 08:52:48,454 >>   Num Epochs = 3\n","[INFO|trainer.py:2111] 2024-05-26 08:52:48,454 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:2114] 2024-05-26 08:52:48,454 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2115] 2024-05-26 08:52:48,454 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2116] 2024-05-26 08:52:48,454 >>   Total optimization steps = 8,142\n","[INFO|trainer.py:2117] 2024-05-26 08:52:48,456 >>   Number of trainable parameters = 335,170,588\n","{'loss': 0.1522, 'grad_norm': 0.33201444149017334, 'learning_rate': 2.815770081061164e-05, 'epoch': 0.18}\n","  6% 500/8142 [09:28<2:25:21,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 09:02:16,833 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-500\n","[INFO|configuration_utils.py:472] 2024-05-26 09:02:16,842 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 09:02:26,586 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 09:02:26,596 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 09:02:26,604 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-500/special_tokens_map.json\n","{'loss': 0.1042, 'grad_norm': 0.37682241201400757, 'learning_rate': 2.631540162122329e-05, 'epoch': 0.37}\n"," 12% 1000/8142 [19:28<2:15:30,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 09:12:17,426 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1000\n","[INFO|configuration_utils.py:472] 2024-05-26 09:12:17,433 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 09:12:27,384 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 09:12:27,395 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 09:12:27,403 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.0941, 'grad_norm': 0.36213424801826477, 'learning_rate': 2.447310243183493e-05, 'epoch': 0.55}\n"," 18% 1500/8142 [29:31<2:06:23,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 09:22:20,151 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1500\n","[INFO|configuration_utils.py:472] 2024-05-26 09:22:20,158 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 09:22:31,464 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 09:22:31,847 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 09:22:31,860 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.09, 'grad_norm': 0.6401007175445557, 'learning_rate': 2.2630803242446574e-05, 'epoch': 0.74}\n"," 25% 2000/8142 [39:40<1:56:42,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 09:32:28,597 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2000\n","[INFO|configuration_utils.py:472] 2024-05-26 09:32:28,605 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 09:32:35,742 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 09:32:35,764 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 09:32:35,780 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.0883, 'grad_norm': 0.5731452107429504, 'learning_rate': 2.078850405305822e-05, 'epoch': 0.92}\n"," 31% 2500/8142 [50:18<1:47:07,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 09:43:06,937 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2500\n","[INFO|configuration_utils.py:472] 2024-05-26 09:43:06,944 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 09:43:13,943 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 09:43:13,957 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 09:43:13,963 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.0827, 'grad_norm': 0.38871780037879944, 'learning_rate': 1.894620486366986e-05, 'epoch': 1.11}\n"," 37% 3000/8142 [1:00:49<1:37:23,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 09:53:38,002 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3000\n","[INFO|configuration_utils.py:472] 2024-05-26 09:53:38,015 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 09:53:47,875 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 09:53:47,915 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 09:53:47,924 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.0764, 'grad_norm': 0.5721294283866882, 'learning_rate': 1.7103905674281504e-05, 'epoch': 1.29}\n"," 43% 3500/8142 [1:10:54<1:28:36,  1.15s/it][INFO|trainer.py:3444] 2024-05-26 10:03:42,805 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3500\n","[INFO|configuration_utils.py:472] 2024-05-26 10:03:42,813 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 10:03:52,200 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 10:03:52,209 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 10:03:52,759 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.0766, 'grad_norm': 0.4594317376613617, 'learning_rate': 1.5261606484893148e-05, 'epoch': 1.47}\n"," 49% 4000/8142 [1:20:47<1:19:03,  1.15s/it][INFO|trainer.py:3444] 2024-05-26 10:13:36,045 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4000\n","[INFO|configuration_utils.py:472] 2024-05-26 10:13:36,055 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 10:13:44,142 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 10:13:44,153 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 10:13:44,161 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.0758, 'grad_norm': 0.414196252822876, 'learning_rate': 1.341930729550479e-05, 'epoch': 1.66}\n"," 55% 4500/8142 [1:30:47<1:09:11,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 10:23:36,341 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4500\n","[INFO|configuration_utils.py:472] 2024-05-26 10:23:36,349 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 10:23:43,744 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 10:23:43,753 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 10:23:43,760 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.0738, 'grad_norm': 0.6407564878463745, 'learning_rate': 1.1577008106116435e-05, 'epoch': 1.84}\n"," 61% 5000/8142 [1:40:45<59:53,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 10:33:33,849 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5000\n","[INFO|configuration_utils.py:472] 2024-05-26 10:33:33,857 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 10:33:42,681 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 10:33:42,691 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 10:33:42,702 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.0718, 'grad_norm': 0.3749491274356842, 'learning_rate': 9.734708916728076e-06, 'epoch': 2.03}\n"," 68% 5500/8142 [1:50:46<50:27,  1.15s/it][INFO|trainer.py:3444] 2024-05-26 10:43:34,951 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5500\n","[INFO|configuration_utils.py:472] 2024-05-26 10:43:34,962 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 10:43:42,564 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 10:43:42,601 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 10:43:42,610 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.0596, 'grad_norm': 1.0368480682373047, 'learning_rate': 7.89240972733972e-06, 'epoch': 2.21}\n"," 74% 6000/8142 [2:00:50<41:00,  1.15s/it][INFO|trainer.py:3444] 2024-05-26 10:53:39,008 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6000\n","[INFO|configuration_utils.py:472] 2024-05-26 10:53:39,016 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 10:53:47,031 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 10:53:47,137 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 10:53:47,146 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.0584, 'grad_norm': 0.5892940759658813, 'learning_rate': 6.050110537951364e-06, 'epoch': 2.39}\n"," 80% 6500/8142 [2:11:01<31:24,  1.15s/it][INFO|trainer.py:3444] 2024-05-26 11:03:50,380 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6500\n","[INFO|configuration_utils.py:472] 2024-05-26 11:03:50,390 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 11:04:00,231 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 11:04:00,239 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 11:04:00,245 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.0564, 'grad_norm': 0.377192884683609, 'learning_rate': 4.2078113485630066e-06, 'epoch': 2.58}\n"," 86% 7000/8142 [2:20:58<21:41,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 11:13:47,278 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7000\n","[INFO|configuration_utils.py:472] 2024-05-26 11:13:47,285 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 11:13:54,854 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 11:13:54,877 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 11:13:54,894 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.0569, 'grad_norm': 0.4458250105381012, 'learning_rate': 2.3655121591746504e-06, 'epoch': 2.76}\n"," 92% 7500/8142 [2:31:10<12:14,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 11:23:58,963 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7500\n","[INFO|configuration_utils.py:472] 2024-05-26 11:23:58,973 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 11:24:09,574 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7500/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 11:24:09,588 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 11:24:09,594 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.0569, 'grad_norm': 0.3789461553096771, 'learning_rate': 5.232129697862933e-07, 'epoch': 2.95}\n"," 98% 8000/8142 [2:41:12<02:42,  1.14s/it][INFO|trainer.py:3444] 2024-05-26 11:34:00,975 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8000\n","[INFO|configuration_utils.py:472] 2024-05-26 11:34:00,984 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 11:34:08,915 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 11:34:08,943 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 11:34:08,954 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8000/special_tokens_map.json\n","100% 8142/8142 [2:44:19<00:00,  1.09it/s][INFO|trainer.py:3444] 2024-05-26 11:37:07,921 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8142\n","[INFO|configuration_utils.py:472] 2024-05-26 11:37:07,930 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8142/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 11:37:16,595 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8142/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 11:37:16,610 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8142/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 11:37:16,616 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/checkpoint-8142/special_tokens_map.json\n","[INFO|trainer.py:2358] 2024-05-26 11:37:36,203 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 9887.748, 'train_samples_per_second': 13.171, 'train_steps_per_second': 0.823, 'train_loss': 0.0792212023017038, 'epoch': 3.0}\n","100% 8142/8142 [2:44:47<00:00,  1.21s/it]\n","[INFO|trainer.py:3444] 2024-05-26 11:37:36,230 >> Saving model checkpoint to bert_large_uncased_goemotions_3_epochs_may_be_better\n","[INFO|configuration_utils.py:472] 2024-05-26 11:37:36,241 >> Configuration saved in bert_large_uncased_goemotions_3_epochs_may_be_better/config.json\n","[INFO|modeling_utils.py:2621] 2024-05-26 11:37:45,481 >> Model weights saved in bert_large_uncased_goemotions_3_epochs_may_be_better/model.safetensors\n","[INFO|tokenization_utils_base.py:2519] 2024-05-26 11:37:45,496 >> tokenizer config file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2528] 2024-05-26 11:37:45,505 >> Special tokens file saved in bert_large_uncased_goemotions_3_epochs_may_be_better/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               = 28260071GF\n","  train_loss               =     0.0792\n","  train_runtime            = 2:44:47.74\n","  train_samples            =      43410\n","  train_samples_per_second =     13.171\n","  train_steps_per_second   =      0.823\n","05/26/2024 11:37:45 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:805] 2024-05-26 11:37:45,615 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, text. If sentence, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3753] 2024-05-26 11:37:45,694 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3755] 2024-05-26 11:37:45,695 >>   Num examples = 5426\n","[INFO|trainer.py:3758] 2024-05-26 11:37:45,695 >>   Batch size = 8\n","100% 679/679 [02:20<00:00,  4.83it/s]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_f1                 =     0.5942\n","  eval_loss               =     0.0853\n","  eval_runtime            = 0:02:21.00\n","  eval_samples            =       5426\n","  eval_samples_per_second =     38.482\n","  eval_steps_per_second   =      4.816\n","05/26/2024 11:40:06 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:805] 2024-05-26 11:40:06,755 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, text. If sentence, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3753] 2024-05-26 11:40:06,759 >> ***** Running Prediction *****\n","[INFO|trainer.py:3755] 2024-05-26 11:40:06,759 >>   Num examples = 5427\n","[INFO|trainer.py:3758] 2024-05-26 11:40:06,759 >>   Batch size = 8\n","100% 679/679 [02:18<00:00,  4.90it/s]\n","05/26/2024 11:42:25 - INFO - __main__ - ***** Predict results *****\n","05/26/2024 11:42:25 - INFO - __main__ - Predict results saved at bert_large_uncased_goemotions_3_epochs_may_be_better/predict_results.txt\n","[INFO|modelcard.py:449] 2024-05-26 11:42:26,256 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.5942342654315643}]}\n"]}]},{"cell_type":"code","source":["# Evaluation for multi-label classification\n","\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import json\n","import re\n","\n","num_classes = 28\n","true_labels = []\n","pred_labels = []\n","with open(\"data/test.json\", \"r\") as fin:\n","    for line in fin:\n","        multi_hot_vector = [0]*num_classes\n","        labels = json.loads(line)[\"labels\"]\n","        for label in labels:\n","            multi_hot_vector[int(label)] = 1\n","        true_labels.append(multi_hot_vector.copy())\n","\n","df_test_preds = pd.read_csv(\"bert_large_uncased_goemotions_3_epochs_may_be_better/predict_results.txt\", sep=\"\\t\")\n","pred_labels_as_str = df_test_preds[\"prediction\"].tolist()\n","for line in pred_labels_as_str:\n","    multi_hot_vector = [0]*num_classes\n","    labels = json.loads(re.sub(\"'\", \"\\\"\", line))\n","    for label in labels:\n","        multi_hot_vector[int(label)] = 1\n","    pred_labels.append(multi_hot_vector.copy())\n","print(classification_report(true_labels, pred_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716723783098,"user_tz":-120,"elapsed":4497,"user":{"displayName":"Raul Fuente Arambarri","userId":"03229099377457333376"}},"outputId":"c2507cd1-11d1-4c54-bdf6-e13f7c15580a","id":"04aIyduVF4aH"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.69      0.72      0.70       504\n","           1       0.78      0.86      0.82       264\n","           2       0.58      0.46      0.51       198\n","           3       0.47      0.26      0.34       320\n","           4       0.52      0.33      0.40       351\n","           5       0.52      0.41      0.46       135\n","           6       0.54      0.39      0.45       153\n","           7       0.55      0.50      0.52       284\n","           8       0.67      0.40      0.50        83\n","           9       0.55      0.23      0.33       151\n","          10       0.45      0.39      0.42       267\n","          11       0.57      0.37      0.45       123\n","          12       0.76      0.35      0.48        37\n","          13       0.58      0.35      0.44       103\n","          14       0.71      0.64      0.68        78\n","          15       0.96      0.89      0.92       352\n","          16       0.00      0.00      0.00         6\n","          17       0.69      0.60      0.64       161\n","          18       0.80      0.84      0.82       238\n","          19       0.50      0.26      0.34        23\n","          20       0.65      0.51      0.57       186\n","          21       0.75      0.19      0.30        16\n","          22       0.45      0.17      0.24       145\n","          23       0.00      0.00      0.00        11\n","          24       0.62      0.71      0.67        56\n","          25       0.64      0.57      0.60       156\n","          26       0.59      0.52      0.55       141\n","          27       0.71      0.57      0.63      1787\n","\n","   micro avg       0.67      0.54      0.60      6329\n","   macro avg       0.58      0.45      0.49      6329\n","weighted avg       0.65      0.54      0.58      6329\n"," samples avg       0.59      0.56      0.57      6329\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WFSDO-z5tgGp"},"execution_count":null,"outputs":[]}]}